{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model architecture\n",
    "\"\"\"\n",
    "nn_arch = [\n",
    "    {\"input_dim\": 2, \"output\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output\": 1, \"activation\": \"sigmoid\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initiate model based on pre-specified architecture \n",
    "\"\"\"\n",
    "def init_layers(nn_arch, seed = 83):\n",
    "    param_cache = {}\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for layer_id,layer in enumerate(nn_arch):\n",
    "        input_dim = layer['input_dim']\n",
    "        output_dim = layer['output']\n",
    "        \n",
    "        param_cache[\"W\" + str(layer_id)] = np.random.randn(output_dim,input_dim)*0.1 #normal(0,0.1) Low variance normal distribution, with 0 mean\n",
    "        param_cache[\"B\" + str(layer_id)] = np.random.randn(output_dim, 1)*0.1\n",
    "    return param_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Activation functions and first derivatives\n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "    \n",
    "def relu(X):\n",
    "    return np.maximum(0,X)\n",
    "    \n",
    "def sigmoid_backward(dZ_curr, A_curr):\n",
    "    return sigmoid(A_curr)*(1-sigmoid(A_curr))*dZ_curr\n",
    "    \n",
    "def relu_backward(dZ_curr, A_curr):\n",
    "    X = np.copy(dZ_curr)\n",
    "    X[A_curr <= 0] = 0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary cross-entropy (Binary classification)\n",
    "\"\"\"\n",
    "Scoring and accuracy metrics\n",
    "\"\"\"\n",
    "def error_cost(output,target):\n",
    "    m = target.shape[1]\n",
    "    #print(m)\n",
    "    if m != 0:\n",
    "        x = -1/m * (np.dot(target,np.log(output).T) + np.dot((1-target),np.log(1-output).T))\n",
    "    #print(output)\n",
    "    return np.squeeze(x)\n",
    "    \n",
    "def nn_accuracy(output,target):\n",
    "    op = prob_to_class(output)\n",
    "    return (op == target).all(axis=0).mean()\n",
    "    \n",
    "def prob_to_class(output):\n",
    "    op = np.copy(output)\n",
    "    op[op>0.5] = 1\n",
    "    op[op<=0.5] = 0\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Single step of forward propagation\n",
    "\"\"\"\n",
    "def single_forward_prop(W_curr, B_curr, Z_prev, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        forward_activation = sigmoid\n",
    "    elif activation == 'relu':\n",
    "        forward_activation = relu\n",
    "    A_curr = np.dot(W_curr, Z_prev) + B_curr\n",
    "    \n",
    "    return A_curr, forward_activation(A_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete forward propagation\n",
    "\"\"\"\n",
    "def full_forward_prop(nn_input, param_cache, nn_arch):\n",
    "    memory_cache = {}\n",
    "    A_curr = nn_input\n",
    "    \n",
    "    for layer_id, layer in enumerate(nn_arch):\n",
    "        Z_prev = A_curr\n",
    "        W_curr = param_cache[\"W\" + str(layer_id)]\n",
    "        B_curr = param_cache[\"B\" + str(layer_id)]\n",
    "        activation = layer['activation']\n",
    "        A_curr, Z_curr = single_forward_prop(W_curr, B_curr, Z_prev, activation)\n",
    "        \n",
    "        memory_cache[\"A\" + str(layer_id)] = A_curr\n",
    "        memory_cache[\"Z\" + str(layer_id)] = Z_prev\n",
    "        \n",
    "    return Z_curr, memory_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Single step of backward propagation\n",
    "\"\"\"\n",
    "def single_back_prop(back_input, A_curr, W_curr, B_curr, Z_prev, activation):\n",
    "    m = back_input.shape[1]\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        backward_activation = sigmoid_backward\n",
    "    elif activation == 'relu':\n",
    "        backward_activation = relu_backward\n",
    "    \n",
    "    delta_A = backward_activation(back_input, A_curr)\n",
    "    \n",
    "    W_err = np.dot(delta_A, Z_prev.T)/m\n",
    "    B_err = np.sum(delta_A, axis = 1, keepdims = True)/m\n",
    "    back_input_ = np.dot(W_curr.T, delta_A)\n",
    "    \n",
    "    return W_err, B_err, back_input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete backward propagation\n",
    "\"\"\"\n",
    "def full_back_prop(param_cache, memory_cache, nn_arch, output, target):\n",
    "    back_input = -1*(np.divide(target,output) - np.divide((1-target),(1-output)))\n",
    "    \n",
    "    for layer_id, layer in reversed(list(enumerate(nn_arch))):\n",
    "        A_curr = memory_cache[\"A\" + str(layer_id)]\n",
    "        Z_prev = memory_cache[\"Z\" + str(layer_id)]\n",
    "        \n",
    "        W_curr = param_cache[\"W\" + str(layer_id)]\n",
    "        B_curr = param_cache[\"B\" + str(layer_id)]\n",
    "        \n",
    "        activation = layer['activation']\n",
    "        \n",
    "        W_err, B_err, back_input_ = single_back_prop(back_input, A_curr, W_curr, B_curr, Z_prev, activation)\n",
    "        back_input = back_input_\n",
    "        \n",
    "        param_cache[\"dW\" + str(layer_id)] = W_err\n",
    "        param_cache[\"dB\" + str(layer_id)] = B_err\n",
    "        \n",
    "    return param_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modifying parameters based on learning rate; end of one iteration\n",
    "\"\"\"\n",
    "def update_params(nn_arch, param_cache, learning_rate):\n",
    "    #m = target.shape[1]\n",
    "    for layer_id, layer in enumerate(nn_arch):\n",
    "        param_cache[\"W\" + str(layer_id)] -= learning_rate*param_cache[\"dW\" + str(layer_id)]\n",
    "        param_cache[\"B\" + str(layer_id)] -= learning_rate*param_cache[\"dB\" + str(layer_id)]\n",
    "        \n",
    "    return param_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main neural net training wrapper function\n",
    "\"\"\"\n",
    "def train_nn(nn_arch, X_train, y_train, learning_rate, epochs, score_logs = True):\n",
    "    error_history = []\n",
    "    accuracy_history = []\n",
    "    param_cache = init_layers(nn_arch)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        output, memory_cache = full_forward_prop(X_train, param_cache, nn_arch)\n",
    "        param_cache = full_back_prop(param_cache, memory_cache, nn_arch, output, y_train)\n",
    "        param_cache = update_params(nn_arch, param_cache, learning_rate)\n",
    "        #output, memory_cache = full_forward_prop(X_train, param_cache, nn_arch)\n",
    "        err = error_cost(output, y_train)\n",
    "        accuracy = nn_accuracy(output, y_train)\n",
    "        error_history.append(err)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        if (i%50 == 0):\n",
    "            if(score_logs):\n",
    "                print(\"Iteration: {0:05} - err: {1:.5f} - accuracy: {2:.5f}\".format(i, err, accuracy))\n",
    "    \n",
    "    return param_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# fraction of total observations used in test_set\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Making training and test datasets\n",
    "\"\"\"\n",
    "X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00000 - err: 0.69951 - accuracy: 0.49556\n",
      "Iteration: 00050 - err: 0.69485 - accuracy: 0.49556\n",
      "Iteration: 00100 - err: 0.69103 - accuracy: 0.49556\n",
      "Iteration: 00150 - err: 0.68732 - accuracy: 0.77889\n",
      "Iteration: 00200 - err: 0.68319 - accuracy: 0.77222\n",
      "Iteration: 00250 - err: 0.67818 - accuracy: 0.78667\n",
      "Iteration: 00300 - err: 0.67179 - accuracy: 0.78889\n",
      "Iteration: 00350 - err: 0.66335 - accuracy: 0.78111\n",
      "Iteration: 00400 - err: 0.65177 - accuracy: 0.77778\n",
      "Iteration: 00450 - err: 0.63528 - accuracy: 0.78444\n",
      "Iteration: 00500 - err: 0.61133 - accuracy: 0.79222\n",
      "Iteration: 00550 - err: 0.57676 - accuracy: 0.80111\n",
      "Iteration: 00600 - err: 0.52932 - accuracy: 0.80889\n",
      "Iteration: 00650 - err: 0.47209 - accuracy: 0.81889\n",
      "Iteration: 00700 - err: 0.41453 - accuracy: 0.83444\n",
      "Iteration: 00750 - err: 0.36707 - accuracy: 0.84889\n",
      "Iteration: 00800 - err: 0.33302 - accuracy: 0.85889\n",
      "Iteration: 00850 - err: 0.31067 - accuracy: 0.86667\n",
      "Iteration: 00900 - err: 0.29682 - accuracy: 0.87333\n",
      "Iteration: 00950 - err: 0.28840 - accuracy: 0.87667\n",
      "Iteration: 01000 - err: 0.28344 - accuracy: 0.87556\n",
      "Iteration: 01050 - err: 0.28058 - accuracy: 0.87667\n",
      "Iteration: 01100 - err: 0.27892 - accuracy: 0.87556\n",
      "Iteration: 01150 - err: 0.27792 - accuracy: 0.87556\n",
      "Iteration: 01200 - err: 0.27735 - accuracy: 0.87556\n",
      "Iteration: 01250 - err: 0.27699 - accuracy: 0.87556\n",
      "Iteration: 01300 - err: 0.27673 - accuracy: 0.87556\n",
      "Iteration: 01350 - err: 0.27655 - accuracy: 0.87556\n",
      "Iteration: 01400 - err: 0.27642 - accuracy: 0.87556\n",
      "Iteration: 01450 - err: 0.27633 - accuracy: 0.87556\n",
      "Iteration: 01500 - err: 0.27626 - accuracy: 0.87556\n",
      "Iteration: 01550 - err: 0.27620 - accuracy: 0.87667\n",
      "Iteration: 01600 - err: 0.27615 - accuracy: 0.87667\n",
      "Iteration: 01650 - err: 0.27612 - accuracy: 0.87667\n",
      "Iteration: 01700 - err: 0.27609 - accuracy: 0.87667\n",
      "Iteration: 01750 - err: 0.27608 - accuracy: 0.87778\n",
      "Iteration: 01800 - err: 0.27607 - accuracy: 0.87778\n",
      "Iteration: 01850 - err: 0.27606 - accuracy: 0.87778\n",
      "Iteration: 01900 - err: 0.27604 - accuracy: 0.87778\n",
      "Iteration: 01950 - err: 0.27602 - accuracy: 0.87778\n",
      "Iteration: 02000 - err: 0.27600 - accuracy: 0.87778\n",
      "Iteration: 02050 - err: 0.27599 - accuracy: 0.87778\n",
      "Iteration: 02100 - err: 0.27597 - accuracy: 0.87778\n",
      "Iteration: 02150 - err: 0.27594 - accuracy: 0.87778\n",
      "Iteration: 02200 - err: 0.27592 - accuracy: 0.87778\n",
      "Iteration: 02250 - err: 0.27590 - accuracy: 0.87778\n",
      "Iteration: 02300 - err: 0.27587 - accuracy: 0.87778\n",
      "Iteration: 02350 - err: 0.27585 - accuracy: 0.87778\n",
      "Iteration: 02400 - err: 0.27585 - accuracy: 0.87778\n",
      "Iteration: 02450 - err: 0.27584 - accuracy: 0.87778\n",
      "Iteration: 02500 - err: 0.27582 - accuracy: 0.87778\n",
      "Iteration: 02550 - err: 0.27581 - accuracy: 0.87778\n",
      "Iteration: 02600 - err: 0.27580 - accuracy: 0.87778\n",
      "Iteration: 02650 - err: 0.27579 - accuracy: 0.87778\n",
      "Iteration: 02700 - err: 0.27578 - accuracy: 0.87778\n",
      "Iteration: 02750 - err: 0.27577 - accuracy: 0.87778\n",
      "Iteration: 02800 - err: 0.27576 - accuracy: 0.87778\n",
      "Iteration: 02850 - err: 0.27575 - accuracy: 0.87778\n",
      "Iteration: 02900 - err: 0.27574 - accuracy: 0.87778\n",
      "Iteration: 02950 - err: 0.27573 - accuracy: 0.87778\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Estimating model parameters on the training set\n",
    "\"\"\"\n",
    "#np.seterr(divide='ignore', invalid = 'ignore')\n",
    "params_cache = train_nn(nn_arch, np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), 0.02, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Estimating target values for test input using estimated model parameters\n",
    "\"\"\"\n",
    "output, _ = full_forward_prop(np.transpose(X_test), params_cache, nn_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Target value accuracy when compared to true output\n",
    "\"\"\"\n",
    "accuracy = nn_accuracy(output, np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
